{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tokenizer Fun: From Bytes to BPE\n",
    "\n",
    "A progressive, hands-on journey through the entire tokenization pipeline:  \n",
    "**Raw string â†’ Unicode code points â†’ UTF-8 bytes â†’ BPE tokens**\n",
    "\n",
    "We'll build a tokenizer from scratch and then benchmark it against OpenAI's production tokenizer.\n",
    "\n",
    "> **Heavily inspired by Andrej Karpathy's video: [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import regex as re\n",
    "import tiktoken\n",
    "import collections\n",
    "import json\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Unicode & Byte Inspection Lab\n",
    "\n",
    "Every string in Python 3 is a sequence of **Unicode code points**.  \n",
    "When we encode to UTF-8, each code point expands into **1â€“4 bytes**.\n",
    "\n",
    "This is the foundation of tokenization: before any BPE, the model sees raw bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_string(text):\n",
    "    \"\"\"Show the Unicode code points and UTF-8 byte breakdown for each character.\"\"\"\n",
    "    print(f\"Input: {text!r}\")\n",
    "    print(f\"{'Char':<6} {'Code Point':<12} {'UTF-8 Bytes':<20} {'# Bytes'}\")\n",
    "    print(\"â”€\" * 50)\n",
    "    \n",
    "    total_bytes = 0\n",
    "    for ch in text:\n",
    "        utf8 = ch.encode('utf-8')\n",
    "        total_bytes += len(utf8)\n",
    "        hex_bytes = ' '.join(f'{b:02x}' for b in utf8)\n",
    "        display_ch = ch if ch.isprintable() else repr(ch)\n",
    "        print(f\"{display_ch:<6} U+{ord(ch):04X}       {hex_bytes:<20} {len(utf8)}\")\n",
    "    \n",
    "    print(f\"\\nâ†’ {len(text)} characters = {total_bytes} UTF-8 bytes (expansion: {total_bytes/len(text):.2f}Ã—)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Hello ðŸ ä¸–ç•Œ!'\n",
      "Char   Code Point   UTF-8 Bytes          # Bytes\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "H      U+0048       48                   1\n",
      "e      U+0065       65                   1\n",
      "l      U+006C       6c                   1\n",
      "l      U+006C       6c                   1\n",
      "o      U+006F       6f                   1\n",
      "       U+0020       20                   1\n",
      "ðŸ      U+1F40D       f0 9f 90 8d          4\n",
      "       U+0020       20                   1\n",
      "ä¸–      U+4E16       e4 b8 96             3\n",
      "ç•Œ      U+754C       e7 95 8c             3\n",
      "!      U+0021       21                   1\n",
      "\n",
      "â†’ 11 characters = 18 UTF-8 bytes (expansion: 1.64Ã—)\n"
     ]
    }
   ],
   "source": [
    "# Try different strings to see how byte counts vary!\n",
    "inspect_string(\"Hello ðŸ ä¸–ç•Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'Ù…Ø±Ø­Ø¨Ø§'\n",
      "Char   Code Point   UTF-8 Bytes          # Bytes\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Ù…      U+0645       d9 85                2\n",
      "Ø±      U+0631       d8 b1                2\n",
      "Ø­      U+062D       d8 ad                2\n",
      "Ø¨      U+0628       d8 a8                2\n",
      "Ø§      U+0627       d8 a7                2\n",
      "\n",
      "â†’ 5 characters = 10 UTF-8 bytes (expansion: 2.00Ã—)\n"
     ]
    }
   ],
   "source": [
    "# Emojis and non-Latin scripts use more bytes\n",
    "inspect_string(\"Ù…Ø±Ø­Ø¨Ø§\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'ðŸ‡¯ðŸ‡µðŸ‡ºðŸ‡¸'\n",
      "Char   Code Point   UTF-8 Bytes          # Bytes\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ‡¯      U+1F1EF       f0 9f 87 af          4\n",
      "ðŸ‡µ      U+1F1F5       f0 9f 87 b5          4\n",
      "ðŸ‡º      U+1F1FA       f0 9f 87 ba          4\n",
      "ðŸ‡¸      U+1F1F8       f0 9f 87 b8          4\n",
      "\n",
      "â†’ 4 characters = 16 UTF-8 bytes (expansion: 4.00Ã—)\n"
     ]
    }
   ],
   "source": [
    "# Flag emojis are TWO code points (regional indicators), 4 bytes each = 8 bytes per flag!\n",
    "inspect_string(\"ðŸ‡¯ðŸ‡µðŸ‡ºðŸ‡¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** A single visible character can be 1â€“8+ bytes. This is why raw byte encoding is inefficient and why we need BPE.\n",
    "\n",
    "---\n",
    "## 2. The BPE Algorithm Scratchpad\n",
    "\n",
    "Byte Pair Encoding works by **iteratively merging the most frequent pair** of adjacent tokens into a new single token. This is how GPT-2/3/4, LLaMA, and other LLM tokenizers build their vocabulary.\n",
    "\n",
    "### Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\"Count frequency of each adjacent pair in the token list.\"\"\"\n",
    "    counts = collections.Counter()\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, new_id):\n",
    "    \"\"\"Replace every occurrence of `pair` in `ids` with `new_id`.\"\"\"\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and (ids[i], ids[i + 1]) == pair:\n",
    "            new_ids.append(new_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the tokenizer\n",
    "\n",
    "Let's train on a small corpus and watch the merges happen step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = (\n",
    "    \"The tokenizer splits text into tokens. Tokens are the atoms of language models. \"\n",
    "    \"Language models predict the next token. The better the tokenizer, the better the model.\"\n",
    ")\n",
    "\n",
    "NUM_MERGES = 20\n",
    "\n",
    "# Start from raw UTF-8 bytes\n",
    "tokens = list(training_text.encode('utf-8'))\n",
    "original_len = len(tokens)\n",
    "\n",
    "# Build initial vocab: one entry per byte value (0â€“255)\n",
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "merges = {}  # (pair) -> new_id\n",
    "\n",
    "# Track history for visualization\n",
    "history = [{'step': 0, 'length': len(tokens)}]\n",
    "\n",
    "print(f\"Starting with {original_len} raw bytes\\n\")\n",
    "print(f\"{'Step':<6} {'Merged Pair':<25} {'New Token':<15} {'Freq':<6} {'Length':<8} {'Compression'}\")\n",
    "print(\"â”€\" * 80)\n",
    "\n",
    "for i in range(NUM_MERGES):\n",
    "    stats = get_stats(tokens)\n",
    "    if not stats:\n",
    "        break\n",
    "    top_pair = max(stats, key=stats.get)\n",
    "    top_freq = stats[top_pair]\n",
    "    new_id = 256 + i\n",
    "    tokens = merge(tokens, top_pair, new_id)\n",
    "    merges[top_pair] = new_id\n",
    "    vocab[new_id] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "    \n",
    "    history.append({'step': i + 1, 'length': len(tokens)})\n",
    "    \n",
    "    p0 = vocab[top_pair[0]].decode('utf-8', errors='replace')\n",
    "    p1 = vocab[top_pair[1]].decode('utf-8', errors='replace')\n",
    "    merged = vocab[new_id].decode('utf-8', errors='replace')\n",
    "    ratio = original_len / len(tokens)\n",
    "    print(f\"#{i+1:<5} {repr(p0) + ' + ' + repr(p1):<25} â†’ {repr(merged):<13} {top_freq:<6} {len(tokens):<8} {ratio:.2f}Ã—\")\n",
    "\n",
    "print(f\"\\nâœ… Final: {original_len} bytes â†’ {len(tokens)} tokens ({original_len/len(tokens):.2f}Ã— compression)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the compression over merge steps\n",
    "steps = [h['step'] for h in history]\n",
    "lengths = [h['length'] for h in history]\n",
    "ratios = [original_len / l for l in lengths]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4.5))\n",
    "\n",
    "ax1.plot(steps, lengths, 'o-', color='#7c3aed', markersize=5, linewidth=2)\n",
    "ax1.fill_between(steps, lengths, alpha=0.1, color='#7c3aed')\n",
    "ax1.set_xlabel('Merge Step')\n",
    "ax1.set_ylabel('Sequence Length')\n",
    "ax1.set_title('Sequence Length vs Merge Step')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.bar(steps, ratios, color='#0ea5e9', edgecolor='#0284c7', alpha=0.8)\n",
    "ax2.set_xlabel('Merge Step')\n",
    "ax2.set_ylabel('Compression Ratio (Ã—)')\n",
    "ax2.set_title('Compression Ratio vs Merge Step')\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** Each merge reduces the sequence length. With just 20 merges on a tiny corpus, we already get meaningful compression. Production tokenizers use ~100k merges on massive corpora.\n",
    "\n",
    "---\n",
    "## 3. Regex Splitter Visualization\n",
    "\n",
    "Before running BPE, production tokenizers **pre-split** the input using regex patterns. This prevents \"cross-category\" merges â€” for example, we don't want `\"dog.\"` to become a single token distinct from `\"dog\"`.\n",
    "\n",
    "GPT-2 and GPT-4 use different splitting patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual patterns used by GPT-2 and GPT-4 tokenizers\n",
    "GPT2_PATTERN = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "def show_splits(text, pattern_name, pattern):\n",
    "    \"\"\"Show how a regex pattern splits the input text.\"\"\"\n",
    "    pieces = re.findall(pattern, text)\n",
    "    print(f\"{pattern_name} â€” {len(pieces)} pieces:\")\n",
    "    displayed = [repr(p) for p in pieces]\n",
    "    print(f\"  {' | '.join(displayed)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"def hello_world():\n",
    "    print(\"Hello, world!\")\n",
    "    return 42 + 3.14\"\"\"\n",
    "\n",
    "print(f\"Input: {test_text!r}\\n\")\n",
    "show_splits(test_text, \"GPT-2 Pattern\", GPT2_PATTERN)\n",
    "show_splits(test_text, \"GPT-4 Pattern\", GPT4_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions â€” notice how the patterns handle apostrophes\n",
    "contraction_text = \"I'm don't they'll we've he'd she's\"\n",
    "\n",
    "print(f\"Input: {contraction_text!r}\\n\")\n",
    "show_splits(contraction_text, \"GPT-2 Pattern\", GPT2_PATTERN)\n",
    "show_splits(contraction_text, \"GPT-4 Pattern\", GPT4_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers â€” GPT-4 splits numbers into groups of 3 digits\n",
    "number_text = \"The price is $12345.67 and pi is 3.14159265\"\n",
    "\n",
    "print(f\"Input: {number_text!r}\\n\")\n",
    "show_splits(number_text, \"GPT-2 Pattern\", GPT2_PATTERN)\n",
    "show_splits(number_text, \"GPT-4 Pattern\", GPT4_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** GPT-4's pattern splits numbers into 1â€“3 digit groups (helping with arithmetic), while GPT-2 keeps entire numbers together. Both patterns separate contractions from their base words.\n",
    "\n",
    "---\n",
    "## 4. The \"Glitch Token\" Investigator\n",
    "\n",
    "The **\"SolidGoldMagikarp\" phenomenon**: some tokens exist in the vocabulary but were **never (or rarely) seen during model training**. When fed to the model, they can cause **bizarre hallucinations** because their embeddings are essentially random noise.\n",
    "\n",
    "### How does this happen?\n",
    "\n",
    "1. The **tokenizer** is trained on a large corpus (e.g., Common Crawl + Reddit)\n",
    "2. Frequent substrings like Reddit usernames become single tokens\n",
    "3. The **model** is then trained on a different (possibly filtered) dataset\n",
    "4. If a token from step 2 never appears in step 3's data, its embedding is never updated\n",
    "5. Result: the token exists but has a random, untrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# Compare \"normal\" strings vs potentially \"glitchy\" strings\n",
    "test_strings = [\n",
    "    # Potentially glitchy (long strings that compress into few tokens)\n",
    "    'SolidGoldMagikarp',\n",
    "    ' RandomRedditor',\n",
    "    'cloneembedaliaboraliaboraliabor',\n",
    "    'TheNitrome',\n",
    "    'DragonMaworking',\n",
    "    # Normal strings for comparison\n",
    "    'Hello world',\n",
    "    'The quick brown fox',\n",
    "    'machine learning',\n",
    "]\n",
    "\n",
    "print(f\"{'Input String':<35} {'Tokens':<8} {'Chars/Tok':<10} {'Token Breakdown'}\")\n",
    "print(\"â”€\" * 90)\n",
    "\n",
    "for s in test_strings:\n",
    "    toks = enc.encode(s)\n",
    "    chars_per_tok = len(s) / len(toks)\n",
    "    pieces = [repr(enc.decode([t])) for t in toks]\n",
    "    flag = \" âš ï¸\" if chars_per_tok > 8 else \"\"\n",
    "    print(f\"{s:<35} {len(toks):<8} {chars_per_tok:<10.1f} {' '.join(pieces)}{flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token frequency distribution from a sample corpus\n",
    "sample_corpus = (\n",
    "    \"The quick brown fox jumps over the lazy dog. Machine learning models process text by first \"\n",
    "    \"converting it into tokens. Each token is an integer that maps to a piece of text. Common words \"\n",
    "    \"like the and is appear frequently, while rare technical terms appear infrequently. The \"\n",
    "    \"tokenizer must handle all of these cases efficiently. Natural language processing has evolved \"\n",
    "    \"significantly with the introduction of transformer-based architectures.\"\n",
    ")\n",
    "\n",
    "corpus_tokens = enc.encode(sample_corpus)\n",
    "token_freq = collections.Counter(corpus_tokens)\n",
    "\n",
    "# Top 20 tokens\n",
    "top_tokens = token_freq.most_common(20)\n",
    "labels = [repr(enc.decode([t])) for t, _ in top_tokens]\n",
    "freqs = [f for _, f in top_tokens]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(range(len(labels)), freqs, color='#0ea5e9', edgecolor='#0284c7', alpha=0.8)\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_yticklabels(labels, fontfamily='monospace', fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_title('Token Frequency Distribution (Top 20)')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "for bar, freq in zip(ax.patches, freqs):\n",
    "    ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height() / 2,\n",
    "            str(freq), va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** Token frequency follows a power law. A few tokens (spaces, common words) dominate, while most tokens are rare. Glitch tokens are the extreme case: they exist in the vocab but have zero or near-zero frequency in the model's training data.\n",
    "\n",
    "---\n",
    "## 5. Tiktoken Benchmarking\n",
    "\n",
    "Let's build an encoder/decoder from our BPE merges and compare against OpenAI's `cl100k_base` (used by GPT-4) across different content types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode(text, merges):\n",
    "    \"\"\"Encode text using our trained BPE merges.\"\"\"\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        # Find the pair with the lowest merge index (earliest learned merge)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        tokens = merge(tokens, pair, merges[pair])\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def bpe_decode(ids, vocab):\n",
    "    \"\"\"Decode token ids back to a string.\"\"\"\n",
    "    raw = b''.join(vocab[idx] for idx in ids)\n",
    "    return raw.decode('utf-8', errors='replace')\n",
    "\n",
    "\n",
    "# Sanity check: round-trip encoding/decoding\n",
    "test_str = \"The tokenizer splits text into tokens.\"\n",
    "encoded = bpe_encode(test_str, merges)\n",
    "decoded = bpe_decode(encoded, vocab)\n",
    "assert decoded == test_str, f\"Round-trip failed: {decoded!r} != {test_str!r}\"\n",
    "print(f\"âœ… Round-trip passed: {test_str!r} â†’ {len(encoded)} tokens â†’ {decoded!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark across content types\n",
    "benchmarks = {\n",
    "    'English Prose': (\n",
    "        \"The quick brown fox jumps over the lazy dog. \"\n",
    "        \"Machine learning models process text by converting it into numerical tokens.\"\n",
    "    ),\n",
    "    'Python Code': textwrap.dedent(\"\"\"\\\n",
    "        def fibonacci(n):\n",
    "            if n <= 1:\n",
    "                return n\n",
    "            return fibonacci(n - 1) + fibonacci(n - 2)\n",
    "\n",
    "        for i in range(10):\n",
    "            print(f\"fib({i}) = {fibonacci(i)}\")\n",
    "    \"\"\"),\n",
    "    'JSON': json.dumps({\n",
    "        \"name\": \"Alice\", \"age\": 30, \"scores\": [95, 87, 92],\n",
    "        \"address\": {\"city\": \"Portland\", \"state\": \"OR\"},\n",
    "    }, indent=2),\n",
    "    'YAML': textwrap.dedent(\"\"\"\\\n",
    "        name: Alice\n",
    "        age: 30\n",
    "        scores: [95, 87, 92]\n",
    "        address:\n",
    "          city: Portland\n",
    "          state: OR\n",
    "    \"\"\"),\n",
    "    'Multilingual': \"Hello world! ã“ã‚“ã«ã¡ã¯ä¸–ç•Œï¼Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…! ðŸŒðŸš€\",\n",
    "    'Arithmetic': \"1234 + 5678 = 6912, 9999 * 1001 = 10008999\",\n",
    "}\n",
    "\n",
    "print(f\"Our BPE: {NUM_MERGES} merges (vocab = {256 + NUM_MERGES}) vs GPT-4 cl100k_base (~100k merges)\\n\")\n",
    "print(f\"{'Content Type':<18} {'Chars':<7} {'Bytes':<7} {'Our BPE':<12} {'cl100k':<12} {'Ratio'}\")\n",
    "print(\"â”€\" * 70)\n",
    "\n",
    "our_totals, tik_totals, categories = [], [], []\n",
    "\n",
    "for name, text in benchmarks.items():\n",
    "    n_bytes = len(text.encode('utf-8'))\n",
    "    our_n = len(bpe_encode(text, merges))\n",
    "    tik_n = len(enc.encode(text))\n",
    "    our_totals.append(our_n)\n",
    "    tik_totals.append(tik_n)\n",
    "    categories.append(name)\n",
    "    \n",
    "    winner = \"â†\" if our_n <= tik_n else \"\"\n",
    "    print(f\"{name:<18} {len(text):<7} {n_bytes:<7} {our_n:<12} {tik_n:<12} {our_n/tik_n:.1f}Ã— {winner}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width / 2, our_totals, width, label=f'Our BPE ({NUM_MERGES} merges)',\n",
    "       color='#7c3aed', alpha=0.8)\n",
    "ax.bar(x + width / 2, tik_totals, width, label='GPT-4 cl100k_base',\n",
    "       color='#0ea5e9', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Content Type')\n",
    "ax.set_ylabel('Token Count')\n",
    "ax.set_title('Token Count: Our BPE vs GPT-4')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories, rotation=20, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-by-token comparison\n",
    "compare_text = \"Hello world! ðŸŒ\"\n",
    "\n",
    "our_toks = bpe_encode(compare_text, merges)\n",
    "tik_toks = enc.encode(compare_text)\n",
    "\n",
    "print(f\"Input: {compare_text!r}\\n\")\n",
    "\n",
    "print(f\"Our BPE ({len(our_toks)} tokens):\")\n",
    "for t in our_toks:\n",
    "    decoded = vocab[t].decode('utf-8', errors='replace')\n",
    "    print(f\"  id={t:<5} â†’ {decoded!r}\")\n",
    "\n",
    "print(f\"\\ncl100k_base ({len(tik_toks)} tokens):\")\n",
    "for t in tik_toks:\n",
    "    decoded = enc.decode([t])\n",
    "    print(f\"  id={t:<5} â†’ {decoded!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Outcomes\n",
    "\n",
    "By completing this notebook, you can now:\n",
    "\n",
    "1. **Demystify Input Processing** â€” Explain the full pipeline: raw string â†’ Unicode code points â†’ UTF-8 bytes â†’ integer tokens\n",
    "\n",
    "2. **Master BPE** â€” Implement and debug Byte Pair Encoding, understanding how vocabulary size impacts sequence length and compute efficiency\n",
    "\n",
    "3. **Diagnose LLM Artifacts** â€” Trace common LLM failures (bad arithmetic, can't reverse strings, \"spelling\" errors) back to tokenization boundaries\n",
    "\n",
    "4. **Differentiate Training vs. Inference** â€” Distinguish between *training a tokenizer* (learning merge rules) and *encoding/decoding text* (applying those rules)\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- Try training with more merges: set `NUM_MERGES = 1000`\n",
    "- Use a larger training corpus\n",
    "- Experiment with different regex split patterns\n",
    "- Watch the full video: [Let's build the GPT Tokenizer â€” Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
